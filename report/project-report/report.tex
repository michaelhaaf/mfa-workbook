\documentclass[11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage{hyperref}
\usepackage{setspace} \doublespacing

% todonotes
\usepackage{xargs}                      % Use more than one optional parameter in a new commands
\usepackage[pdftex,dvipsnames]{xcolor}  % Coloured text etc.
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}

% margins for todonotes
\paperwidth=\dimexpr \paperwidth + 6cm\relax
\oddsidemargin=\dimexpr\oddsidemargin + 3cm\relax
\evensidemargin=\dimexpr\evensidemargin + 3cm\relax
\marginparwidth=\dimexpr \marginparwidth + 3cm\relax

% Definitions of handy macros can go here
\usepackage{subfig}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\renewcommand*\contentsname{Table of Contents}

\begin{document}
\title{MSc. Project Report Winter 2022}
\ShortHeadings{MSc. Project Report Winter 2022, McGill University}{}
\author{Author: Michael Haaf \textit{(michael.haaf@mail.mcgill.ca)} \\ Supervisor: Morgan Sonderegger \textit{(morgan.sonderegger@mcgill.ca)}}

\maketitle
\begin{singlespace}
\tableofcontents
\end{singlespace}

\section{Introduction}

\nocite{*}

Natural language is predominantly a spoken phenomena, while the most commons tools for computational natural language processing are predominantly text based. \change{citations/discussion needed in this section}Tools that represent, store, and process audio signals associated with spoken natural language have not always existed and are not trivial to implement relative to comparable text-based processing tools. The reasons for this are straightforward: text-based data is comparably smaller in storage size and less demsnding in memory to process. Moreover, speech-based data have much more complicated metadeta and preprocessing requirements than text-based data, many of these complications being out of the reach of amateur or non-technically proficient researchers. Speech transcription, aligning transcription with speech audio, audio signal processing, and non-uniform metadata/data storage standards are just some of the complications of speech-based natural language processing compared to text-based natural language processing. Given these obstacles, it is not surprising that more tools are researched, written, and developed for text-based corpora than for speech-based corpora in both natural language processing research and industry.

The availability of speech-based corpora and automated tools to make use of them is increasing\change{use zotera references}\footnote{corpora sources , automated alignment , standard variable measurement }, enabling linguists and speech scientists to study spoken language at a much larger scale than previously possible\footnote{large scale studies}. It remains challenging, however, to surmount the obstacles described above (and developed in the body of this report) without technical know-how in both linguistics and computer science: namely: \change{finish this paragraph}.

The main systems built were\ldots \change{finish this paragraph}

The results were\ldots\change {finish this paragraph}

\section{Literature Review}

This section describes the main speech corpora software that this project work made use of (Montreal Forced Aligner and PolyglotDB). This section will also describe comparable software. \change{finish this section}.

The key contributions of the Polyglot system are to meet scalability and speed expectations (performance in reasonable time as the amount of data grows) while also minimizing scripting through abstraction away from corpus format. Users should need minimal technical skill, and should be able to interact with corpora without understanding particularities of their formats (as for textual corpora in NLTK[11]). Technically-skilled users should be able to avoid rewriting scripts with similar functionality. \change{don't plagiarise this section}

Given that speech corpora are complex and heterogeneous (metadata, database directories, and annotation files can all be highly varied in structure. Dozens of formats have been used to store speech corpora over the past three decades. These factors make studies using data from many corpora practically difficult, with researchers writing extensive scripts to perform similar operations on different corpora, despite substantial structural similarities across all speech corpora. The scripting involved in corpus work is time-consuming for technical users, and a deal-breaker for non-technical users.

Several other systems for management and analysis of speech corpora exist (e.g. [15, 16, 17, 18, 19]), including three systems which are most similar to Polyglot-SCT. Phon [15] is a system for creating and querying corpora. Phon uses a relational database to store data, but does not adopt the annotation graph formalism. Phon is integrated with Praat [20], and allows for a range of acoustic analyses and linguistic analyses (e.g. syllabification) across many languages. LaBB-CAT [16] stores recordings and associated transcriptions as annotation graphs in a relational database. In addition to import, export, and querying, LaBB-CAT can enrich a corpus in various ways (e.g. forced alignment, syllabification), and offers integration with Praat and lexical databases. EMU-SDMS [17] is a system consisting of an R [21] package, to simplify the full pipeline of corpus research to a single environment for data preparation and analysis, and a web application for annotation and file inspection. EMU also uses annotation graphs, which are stored in JSON files, as are subsequent measurements (f0, etc.) made using a signal processing library. Querying is done through a custom query language. Polyglot-SCT differs from other systems in its goals: it is optimized for large-scale studies across many corpora, maximizing scalability, speed, and ease of use. \change{don't plagiarise this section; tweek for relevancy/actually do a review}

\section{Speech Analysis Pipeline}

Technical explanation of the problems raised in previous sections. In this section I explain the problem overall, showing where canonical solutions exist and indicating where my project makes contributions to explain in subsequent sections (particularly: 3.2/3.3/3.4/3.5/3.6). If we have time, hopefully I can make a quick contribution for 3.7, otherwise I'll point the way for future work.

We can take corpora of from the IARPA <++> (link) dataset as a starting point, though the procedure is general for all speech corpora. IARPA corpora are organized in the following manner:
\begin{singlespace}
\begin{verbatim}
corpus/
|-- scripted/
|   |-- reference_materials/
|   |   |   `-- lexicon.txt
|   |   |   `-- lexicon.sub-train.txt
|   |-- training/
|   |   |-- audio/
|   |   |   `-- recording1.sph
|   |   |   `-- recording2.sph
|   |   |   `-- ...
|   |   |-- transcript_roman/
|   |   |   `-- recording1.txt
|   |   |   `-- recording2.txt
|   |   |   `-- ...
\end{verbatim}
\end{singlespace}

The Montreal Forced Aligner assumes its own particular format corpus structure:\footnote{https://montreal-forced-aligner.readthedocs.io/en/latest/user\_guide/formats/corpus\_structure.html}

\begin{singlespace}
\begin{verbatim}
`--pronunciation_dictionary.txt
`--textgrid_corpus/
|   `-- recording1.wav
|   `-- recording1.TextGrid
|   `-- recording2.wav
|   `-- recording2.TextGrid
|   `-- ...
\end{verbatim}
\end{singlespace}

That is, the following steps need to be taken:

- Convert bulk generic audio files to 16kHz .wav files
- Convert bulk .txt transcripts to .TextGrids
- Convert Iarpa corpus lexicon to MFA-ready pronunciation dictionary
- Prepare all of the above for alignment with MFA

Instructions for each step, using the code in this repository, are given below. You can follow the steps with the data contained in the sample-data directory to get a sense of the process. Sample results for this dataset are also given in the sample-data directory.


\subsection{Speech Corpora Acquisition and Organization}

- Download from the internet (discoverability, formats, etc.)
- Occassional need to filter based on what you already have (see the dl.sh script I created that's currently in the google drive)

\subsection{Transcript to `TextGrid` Conversion}

Praat \change{find citation [20]} TextGrids, and allows for structuring ‘tiers’ in the TextGrid into the more meaningful hierarchy defined in the database format (e.g. each phone token belongs to a corresponding word). TextGrid-based formats which are output from various programs are also supported: the MFA [3], Prosodylab-Aligner [4], and FAVE [1] forced aligners, as well as LaBB-CAT. Importers also exist for corpora in BAS Partitur format [24], as well as for the TIMIT and Buckeye corpora [25, 26].

Generally, transcripts for audio corpora are not made available in TextGrid format. The conversion from general transcript files to TextGrids does not exist, since metadata and formatting can vary from transcript to transcript. This project contributes a general method for specifying the input transcript file properties, such that a variety of audio corporas can have their transcripts converted to the textgrid format. A technical description of this work follows:

The Python library `praatio` \change{find citation} has useful utilities for performing this conversion. These include \change{find what these include}. The `txt-to-textgrid.py` script makes use of these utilities to convert a specified directory of `.txt` files to the `.TextGrid` format. The script also converts Iarpa tags to their MFA equivalent along with other syntax related substitutions.

Description/sample usage/results here: \change{complete this paragraph}

\subsection{Audio Signal Processing}

Aligning audio recordings with text transcriptions is a complicated signal processing task. Uniform standards for time frequency are given by file formant standards (what?? maybe do some reading??).

To align generic audio recordings with generic text transcriptions, there are two prerequisites: a known sampling frequency, and a known file format (What??). In this project, the Montreal Forced Aligner is used, which runs optimally using 16kHz sampling frequency and the .wav audio standard. While many other combinations of frequency and format are possible in principle, some standard must be applied in order for measurement to take place, in turn allowing automated alignment to take place. This section concerns the problem of transforming a large set of audio files to a common sampling frequency and a common file format standard.

For example, the IARPA corpus stores audio files using the `.sph` file format standard, with audio recorded at an 8kHz sample rate. This corpus therefore demonstrates a key example of a corpus needing tranformations in both crucial metrics. Conversion from this format to the standard established above requires scripting: while there exist many tools to convert and resample audio formats, there are none that, crucially, (1) convert arbitrary audio formats to a chosen standard (in this case, .wav); (2) resample .wav from arbitrary sampling frequency to a chosen standard (in this case, 16kHz) without altering the pitch nor speed of the audio file; (3) handle gigabytes of audio files in bulk without running into RAM issues on ordinary machines (16GB RAM).

By point of comparison, the Praat tool can do (1) and (2) but not (3), since Praat scripts, in an effort to produce a non-technical user experience, do not offer memory management or lazy/dynamic loading of audio files at runtime which woudl be required to fulfill requirement (3).



\subsection{Pronunciation Dictionary Production}

MFA pronunciation dictionaries are two-column files, where the columns represent a many to many mapping from words to pronunciations. The lexicon from the Cantonese speech corpus seems to be noncompliant with MFA pronunciation dictionaries in two ways: it includes tone markers (which do not correspond to phonemes), and it contains an extra column, containing what I believe to be latin-script transliterations of the utterance pronunciations.

dacite dependency <++> (part of explanation of implementation)

\subsection{Alignment Production}

Note well: the sample-data/ given in this repository (10 randomly chosen ~10second speech files) is no where near enough data to train a performant model. With this data we are simply verifying that there are no syntax/format issues with your workflow. Once that is verified, you will need to acquire more data to train a performant model. See \href{https://memcauliffe.com/how-much-data-do-you-need-for-a-good-mfa-alignment.html}{this blog post} for rough guidelines as to the magnitude of data required to train a model for alignment/general usage.


\subsection{Aligned Speech Corpora Storage}

Putting the aligned .wavs and .textgrids into polyglotDB for further analysis

\subsection{Linguistic Analysis}

Basic demonstration of the formants working for engl, canto, lithu

\subsection{Generalization to New Languages and Corpora}

Implementation details for getting corpuses ready for alignment (scripts, new src modules with flexible business logic for configuration by non experts, unit testing, etc.)

    * [X] review phoneset, syllables, tone rules, etc.
    * [X] tone charter markers
    * [X] multiple lines per pronunciation
    * [X] remove word boundaries ("\#", see e.g.)
    * [X] pipe into Cantonese-wrap-up process

\subsection{PolyglotDB Package Management}

Implementation details of transition to conda: dependency management, creating a development environment, future suport for multiple architectures, integration into development and production usage.

The PolyglotDB package used to be installed using pip. There were problems with external dependencies (Kaldi) requiring manual precompilation.

- canadacompute
- roquefort
- os x
- windows
- intel/amd linux
- that other type of processor which is flash based that i forget

\section{Experiments}

Overview of experiments reproduced/conducted.

\subsection{Reproduction of Existing Experiments}

Allude to the earlier section results. The following will be further developments on the LibriSpeech english dataset to see what else is possible with these tools.

A freshly-imported corpus will result in a database containing at
a minimum the word and phone levels (see Figure 1 right) and
any other information from the corpus’ annotation files. Any
other information is added in the enrichment phase.
Polyglot-SCT databases can be enriched in many ways, by
adding structure and measures that are often used in linguistic
studies. First, new annotations can be created. Larger connected
speech chunks, termed utterances, can be created as parents of
word annotations (as in Fig. 1 right). Utterances are created by
encoding speech versus non-speech elements in each file, then
specifying the minimum duration of non-speech elements corre-
sponding to an utterance boundary. Syllable annotations, which
are parents of phones, can also be created using the maximum
onset algorithm. We plan to add other algorithms for marking
boundaries and for syllabification in the future.
Second, measures based on hierarchical relations can be
calculated and stored. For instance, once utterances and syl-
lables have been created, speech rate can be calculated as sylla-
bles per second in the utterance, and stored as a property of the
utterance. Count and position of lower elements within higher
elements can be encoded, such as syllable position within a
word or number of syllables in a word (properties of the syl-
lable and word, respectively).
Third, properties of lexical items, segments, and metadata
about speakers or sound files can be added—such as from lex-
icons (e.g. frequency, part of speech) or files listing properties
of phones (e.g. phonological features).
Fourth, acoustic measurements from the sound files can
be calculated and stored. Currently, f0 (using Praat or Reaper
[5]), intensity, and formants (using Praat) are supported. Other
acoustic measurements will be added in future work, by incor-porating external tools (e.g. for VOT, pitch accent detection:
[6, 27]) and further integration with Praat.
Finally, ‘relativized’ versions of measures calculated in en-
richment can be calculated. For example, it is often of interest
in phonetic studies to know how long a phone (token) is relative
to its mean duration in the corpus, or an utterance’s speech rate
relative to the speaker’s mean rate.
Anything encoded as part of enrichment is saved and can
be queried in the future. The intended use case for Polyglot-
SCT is for import and enrichment to be done once per corpus.
These steps can be slow (see Table 1), but require minimal in-
put from the user. By contrast, querying and exporting are fast,
and can be done many times, in different studies with different
goals. This design allows users to not repeat work like recalcu-
lating measures (e.g. pitch tracks) for each new study, which is
important for scalability to large-scale studies.
<++> (note: copied! revise!)

\subsection{Formant Analysis}

\subsection{Pitch Analysis}

\subsection{Future Work}

* Imbert Orchard, 2700 hours audiotape (available for purchase on DVD. Uploaded/available for download?)
* Very similar line of work to what I'm thinking: https://langmusecad.wordpress.com/tag/university-of-victoria/ 
* idea?: speech to wordvector tutorial? different in speech and in text, could be something there
* idea?: people as "vectors", output of MFA without polyglot
* note: SPADE uses polyglot under the hood, would be nice to ... 
    * replicate dataset/analyses that are already on SPADE, 
    * make something not just for experts, shows you how to work with your own corpus to see
* idea: time dimensions for datasets not yet examined (related to Peace River corpus)

* analyze pitch: something like the chodroff tutorial (see slack), a working script with known input/output
* 5 or 10 examples like this well documented
* extension with time: Mark Liberman projects ("breakfast experiment"). How to facilitate?
* Actually realize some of these projects and document them for others. Cool end goal.



\section{Conclusions}

Summarize results and contributions, indicate future work, acknowledgements, etc.

\section{Appendix (TODO: properly)}

"Program listings should only be attached as appendices."

\section{Detailed description of the repository (TODO: add other repos worked on)}
\label{sec:repository}

Experiment code can be viewed here: \\ \href{https://github.com/michaelhaaf/mfa-workbook}. The following describes the usage of each component of this code:

\begin{verbatim}
 |-README.md
 |-requirements.txt
 |-scripts/
 | |-bulk_sph_resample
 | |-create_random_subset
 | |-lexicon_to_dict.py
 | |-transcripts_to_textgrids.py
 | |-resample.praat
 |-src/
 | |-dictionary.py
 | |-models.py
 | |-syllable.py
 | |-lexicon.py
 |-test
 | |-test_dictionary.py
 | |-test_models.py
 | |-test_syllable.py
 | |-test_lexicon.py
 | |-test_transcripts_to_textgrids.py
 |-experiments/
 | |-scripts/
 | | |-tutorial_first_steps.py
 | | |-tutorial_enrichment.py
 | | |-tutorial_formants.py
 | | |-tutorial_pitch.py
 | |-analysis/
 | | |-tutorial_first_steps.Rmd
 | | |-tutorial_enrichment.Rmd
 | | |-tutorial_formants.Rmd
 | | |-tutorial_pitch.Rmd
 | |-results/
 | | |-tutorial_first_steps.html
 | | |-tutorial_enrichment.html
 | | |-tutorial_formants.html
 | | |-tutorial_pitch.html
 |-sample-data/
 | |-canto-pd.txt
 | |-lithu-pd.txt
 | |-iarpa-corpora/
 | | |-lithu/
 | | |-canto/
 | |-aligned-corpora/
 | | |-lithu/
 | | |-canto/
\end{verbatim}


\vskip 0.2in
\bibliography{sample}

\end{document}
